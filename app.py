#!/usr/bin/env python3
"""
Esperanto Learning with ChatGPT: Interactive Dataset Explorer
A behavioral economics study analyzing AI-assisted language learning

HuggingFace Spaces deployment for interactive visualization and exploration
of 397 ChatGPT conversations from Esperanto learners.
"""

import gradio as gr
import pandas as pd
import numpy as np
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
from pathlib import Path
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
import json

# Load data
def load_data():
    """Load and prepare the dataset"""
    df = pd.read_csv("data/conversations_final.csv")
    # Filter out conversations with errors
    df = df[df['error'].isna() | (df['error'] == '')]
    return df

# Global data loading
df = load_data()

# Define metrics
METRICS = [
    "cognitive_engagement",
    "metacognitive_awareness",
    "linguistic_production",
    "self_directedness",
    "iterative_refinement",
    "memory_retention",
    "query_sophistication",
    "agency_ownership",
    "overall_learning_quality"
]

COGNITIVE_DEBT_METRICS = ["iterative_refinement", "memory_retention", "metacognitive_awareness"]

def create_distribution_plot():
    """Create interactive distribution plots for all metrics"""
    fig = make_subplots(
        rows=3, cols=3,
        subplot_titles=[m.replace('_', ' ').title() for m in METRICS],
        vertical_spacing=0.12,
        horizontal_spacing=0.1
    )

    for idx, metric in enumerate(METRICS):
        row = idx // 3 + 1
        col = idx % 3 + 1

        # Create histogram
        values = df[metric].dropna()
        fig.add_trace(
            go.Histogram(
                x=values,
                name=metric,
                nbinsx=20,
                marker_color='rgb(55, 83, 109)',
                showlegend=False,
                hovertemplate='Score: %{x}<br>Count: %{y}<extra></extra>'
            ),
            row=row, col=col
        )

        # Add mean line
        mean_val = values.mean()
        fig.add_vline(
            x=mean_val,
            line_dash="dash",
            line_color="red",
            annotation_text=f"Œº={mean_val:.2f}",
            annotation_position="top",
            row=row, col=col
        )

        # Update axes
        fig.update_xaxes(title_text="Score", row=row, col=col, range=[1, 5])
        fig.update_yaxes(title_text="Frequency", row=row, col=col)

    fig.update_layout(
        height=900,
        title_text="Distribution of Learning Metrics (n=397 conversations)",
        title_font_size=16,
        showlegend=False
    )

    return fig

def create_correlation_heatmap():
    """Create interactive correlation matrix"""
    corr_matrix = df[METRICS].corr()

    # Create custom hover text
    hover_text = []
    for i in range(len(METRICS)):
        hover_row = []
        for j in range(len(METRICS)):
            hover_row.append(
                f"{METRICS[i].replace('_', ' ').title()}<br>"
                f"√ó {METRICS[j].replace('_', ' ').title()}<br>"
                f"r = {corr_matrix.iloc[i, j]:.3f}"
            )
        hover_text.append(hover_row)

    fig = go.Figure(data=go.Heatmap(
        z=corr_matrix,
        x=[m.replace('_', ' ').title() for m in METRICS],
        y=[m.replace('_', ' ').title() for m in METRICS],
        colorscale='RdBu',
        zmid=0,
        text=corr_matrix.round(2),
        texttemplate='%{text}',
        textfont={"size": 10},
        hovertext=hover_text,
        hoverinfo='text',
        colorbar=dict(title="Correlation")
    ))

    fig.update_layout(
        title="Metric Correlations<br><sub>Strong correlations (r > 0.6): "
              "Cognitive Engagement ‚Üî Learning Quality (0.72), "
              "Agency ‚Üî Self-Direction (0.68)</sub>",
        title_font_size=16,
        height=700,
        xaxis={'side': 'bottom'},
        yaxis={'autorange': 'reversed'}
    )

    return fig

def create_cognitive_debt_plot():
    """Create cognitive debt indicators visualization"""
    data = []
    colors = ['rgb(93, 164, 214)', 'rgb(255, 144, 14)', 'rgb(44, 160, 101)']

    for idx, metric in enumerate(COGNITIVE_DEBT_METRICS):
        values = df[metric].dropna()
        data.append(go.Box(
            y=values,
            name=metric.replace('_', ' ').title(),
            marker_color=colors[idx],
            boxmean='sd',
            hovertemplate='<b>%{fullData.name}</b><br>Value: %{y:.2f}<extra></extra>'
        ))

    fig = go.Figure(data=data)

    # Add threshold line
    fig.add_hline(
        y=2.5,
        line_dash="dash",
        line_color="red",
        annotation_text="Moderate threshold",
        annotation_position="right"
    )

    # Add interpretation annotations
    interpretations = {
        "Iterative Refinement": "2.91/5: Moderate follow-up depth",
        "Memory Retention": "3.68/5: Good knowledge retention",
        "Metacognitive Awareness": "3.22/5: Moderate self-reflection"
    }

    fig.update_layout(
        title="Cognitive Debt Indicators (MIT 2025 Framework)<br>"
              "<sub>Learners show moderate engagement with above-average memory retention</sub>",
        title_font_size=16,
        yaxis_title="Score (1-5 scale)",
        height=600,
        showlegend=True,
        hovermode='x'
    )

    return fig

def create_cluster_plot():
    """Create learner profile clusters visualization"""
    pca_metrics = METRICS[:6]
    pca_data = df[pca_metrics].dropna()

    # PCA
    pca = PCA(n_components=2)
    components = pca.fit_transform(pca_data)

    # K-means clustering
    kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)
    clusters = kmeans.fit_predict(components)

    # Cluster labels based on analysis
    cluster_names = {
        0: "Low Engagers (25%)",
        1: "Moderate Balanced (38%)",
        2: "High Agency (20%)",
        3: "Production Focused (17%)"
    }

    cluster_labels = [cluster_names[c] for c in clusters]

    # Create scatter plot
    fig = go.Figure(data=go.Scatter(
        x=components[:, 0],
        y=components[:, 1],
        mode='markers',
        marker=dict(
            size=8,
            color=clusters,
            colorscale='Viridis',
            showscale=True,
            colorbar=dict(
                title="Cluster",
                tickvals=[0, 1, 2, 3],
                ticktext=list(cluster_names.values())
            ),
            line=dict(width=0.5, color='white')
        ),
        text=cluster_labels,
        hovertemplate='<b>%{text}</b><br>PC1: %{x:.2f}<br>PC2: %{y:.2f}<extra></extra>'
    ))

    # Add cluster centroids
    centroids = kmeans.cluster_centers_
    fig.add_trace(go.Scatter(
        x=centroids[:, 0],
        y=centroids[:, 1],
        mode='markers',
        marker=dict(
            size=15,
            symbol='star',
            color='red',
            line=dict(width=2, color='white')
        ),
        name='Centroids',
        showlegend=True,
        hovertemplate='Cluster Centroid<extra></extra>'
    ))

    variance_explained = pca.explained_variance_ratio_

    fig.update_layout(
        title=f"Learner Profiles (PCA + K-means Clustering)<br>"
              f"<sub>4 distinct learner types identified from behavioral patterns</sub>",
        title_font_size=16,
        xaxis_title=f"PC1 ({variance_explained[0]:.1%} variance)",
        yaxis_title=f"PC2 ({variance_explained[1]:.1%} variance)",
        height=700,
        hovermode='closest'
    )

    return fig

def create_relationships_plot():
    """Create key relationship scatterplots"""
    fig = make_subplots(
        rows=2, cols=2,
        subplot_titles=[
            'Cognitive Engagement ‚Üí Learning Quality (r=0.72)',
            'Agency ‚Üí Self-Direction (r=0.68)',
            'Linguistic Production ‚Üí Query Quality',
            'Iterative Refinement ‚Üí Memory Retention'
        ],
        vertical_spacing=0.15,
        horizontal_spacing=0.12
    )

    relationships = [
        ('cognitive_engagement', 'overall_learning_quality'),
        ('agency_ownership', 'self_directedness'),
        ('linguistic_production', 'query_sophistication'),
        ('iterative_refinement', 'memory_retention')
    ]

    for idx, (x_metric, y_metric) in enumerate(relationships):
        row = idx // 2 + 1
        col = idx % 2 + 1

        # Scatter plot
        fig.add_trace(
            go.Scatter(
                x=df[x_metric],
                y=df[y_metric],
                mode='markers',
                marker=dict(
                    size=6,
                    color='rgb(55, 83, 109)',
                    opacity=0.5
                ),
                name=f"{x_metric} vs {y_metric}",
                showlegend=False,
                hovertemplate=f'{x_metric.replace("_", " ").title()}: %{{x:.2f}}<br>'
                              f'{y_metric.replace("_", " ").title()}: %{{y:.2f}}<extra></extra>'
            ),
            row=row, col=col
        )

        # Add trend line
        x_clean = df[x_metric].dropna()
        y_clean = df[y_metric].dropna()
        common_idx = df[[x_metric, y_metric]].dropna().index

        if len(common_idx) > 3:
            z = np.polyfit(df.loc[common_idx, x_metric], df.loc[common_idx, y_metric], 1)
            p = np.poly1d(z)
            x_line = np.linspace(df[x_metric].min(), df[x_metric].max(), 100)

            fig.add_trace(
                go.Scatter(
                    x=x_line,
                    y=p(x_line),
                    mode='lines',
                    line=dict(color='red', dash='dash', width=2),
                    name='Trend',
                    showlegend=False,
                    hoverinfo='skip'
                ),
                row=row, col=col
            )

        # Update axes
        fig.update_xaxes(
            title_text=x_metric.replace('_', ' ').title(),
            row=row, col=col,
            range=[1, 5]
        )
        fig.update_yaxes(
            title_text=y_metric.replace('_', ' ').title(),
            row=row, col=col,
            range=[1, 5]
        )

    fig.update_layout(
        height=800,
        title_text="Key Relationships Between Metrics",
        title_font_size=16,
        showlegend=False
    )

    return fig

def create_learning_patterns_plot():
    """Create learning patterns bar chart"""
    patterns = {
        'Declarative Bias': 50,
        'Procedural Focus': 50,
        'High AI Reliance': 32,
        'Balanced Autonomy': 48,
        'High Self-Direction': 20,
        'Translation Queries': 42.5,
        'Grammar Queries': 27.5,
        'Application Queries': 5,
        'Meta-Learning Queries': 2
    }

    categories = ['Learning Style', 'Learning Style', 'AI Dependency', 'AI Dependency',
                  'AI Dependency', 'Query Types', 'Query Types', 'Query Types', 'Query Types']

    df_patterns = pd.DataFrame({
        'Pattern': list(patterns.keys()),
        'Percentage': list(patterns.values()),
        'Category': categories
    })

    fig = px.bar(
        df_patterns,
        x='Percentage',
        y='Pattern',
        color='Category',
        orientation='h',
        title='Learning Patterns Analysis<br><sub>Distribution of learner behaviors and query types</sub>',
        labels={'Percentage': 'Percentage of Learners (%)', 'Pattern': ''},
        color_discrete_map={
            'Learning Style': 'rgb(93, 164, 214)',
            'AI Dependency': 'rgb(255, 144, 14)',
            'Query Types': 'rgb(44, 160, 101)'
        }
    )

    fig.update_traces(
        hovertemplate='<b>%{y}</b><br>%{x:.1f}% of learners<extra></extra>'
    )

    fig.update_layout(
        height=600,
        xaxis_range=[0, 60],
        yaxis={'categoryorder': 'total ascending'}
    )

    return fig

def get_dataset_stats():
    """Generate dataset statistics"""
    stats = f"""
### Dataset Statistics

- **Total Conversations**: {len(df)}
- **Total Participants**: 375
- **Evaluation Success Rate**: 98% (389/397)
- **Date Range**: December 2024
- **Average Messages per Conversation**: {df['n_user_messages'].mean():.1f}

### Metric Summary Statistics

| Metric | Mean | Std | Median | Min | Max |
|--------|------|-----|--------|-----|-----|
"""

    for metric in METRICS:
        mean = df[metric].mean()
        std = df[metric].std()
        median = df[metric].median()
        min_val = df[metric].min()
        max_val = df[metric].max()
        stats += f"| {metric.replace('_', ' ').title()} | {mean:.2f} | {std:.2f} | {median:.2f} | {min_val:.2f} | {max_val:.2f} |\n"

    return stats

def explore_conversation(conv_id_input):
    """Explore individual conversation details"""
    try:
        conv_idx = int(conv_id_input)
        if conv_idx < 0 or conv_idx >= len(df):
            return f"Invalid conversation index. Please enter a number between 0 and {len(df)-1}"

        conv = df.iloc[conv_idx]

        result = f"""
## Conversation #{conv_idx}

**Title**: {conv.get('title', 'N/A')}
**Participant ID**: {conv.get('participant_id', 'N/A')}
**Model**: {conv.get('default_model_slug', 'N/A')}
**Messages**: {conv.get('n_user_messages', 'N/A')}
**Date**: {conv.get('create_time_iso', 'N/A')[:10]}

### Evaluation Scores

| Metric | Score |
|--------|-------|
"""

        for metric in METRICS:
            score = conv.get(metric, 'N/A')
            if isinstance(score, (int, float)):
                result += f"| {metric.replace('_', ' ').title()} | {score:.2f} |\n"
            else:
                result += f"| {metric.replace('_', ' ').title()} | {score} |\n"

        # Add interpretation
        overall_quality = conv.get('overall_learning_quality', 0)
        if overall_quality >= 4:
            interpretation = "**High Quality**: This conversation shows strong learning behaviors with excellent engagement."
        elif overall_quality >= 3:
            interpretation = "**Moderate Quality**: This conversation demonstrates adequate learning with room for improvement."
        else:
            interpretation = "**Low Quality**: This conversation shows minimal learning engagement."

        result += f"\n### Interpretation\n\n{interpretation}"

        return result

    except ValueError:
        return "Please enter a valid conversation index (integer)"

# Create Gradio interface
with gr.Blocks(title="Esperanto Learning Dataset Explorer", theme=gr.themes.Soft()) as demo:
    gr.Markdown("""
# Esperanto Learning with ChatGPT: Behavioral Economics Study
## Interactive Dataset Explorer

**397 ChatGPT conversations** from **375 participants** learning Esperanto, evaluated using validated frameworks from educational psychology and cognitive science literature.

This interactive platform allows you to explore the dataset, visualize learning patterns, and understand how learners interact with AI for language acquisition.

### Key Findings

- **Moderate Cognitive Engagement**: Learners show 2.91/5 iterative refinement with 3.68/5 memory retention
- **Declarative Bias**: 50% prefer fact/translation requests over procedural understanding
- **Strong Correlations**: Cognitive Engagement ‚Üî Learning Quality (r=0.72)
- **Four Learner Profiles**: Low Engagers (25%), Moderate Balanced (38%), High Agency (20%), Production Focused (17%)

---
""")

    with gr.Tabs():
        with gr.Tab("üìä Overview"):
            gr.Markdown(get_dataset_stats())
            gr.Markdown("""
### Evaluation Framework

All metrics derived from **validated frameworks** in educational psychology:
- **Cognitive Load**: Klepsch et al. 2017 (Frontiers in Psychology)
- **Metacognition**: Schraw & Dennison 1994 (MAI)
- **Self-Regulation**: Pintrich et al. 1991 (MSLQ)
- **Linguistic Production**: ACTFL 2024 (FACT)
- **Engagement**: Kahu et al. 2018 (HESES)
- **Critical Thinking**: Liu et al. 2014
- **Cognitive Debt**: MIT Media Lab 2025

### Methodology

- **Model**: GPT-3.5-turbo (temperature 0.1)
- **Processing**: Concurrent evaluation (8 workers)
- **Cost**: ~$1-2 total for 397 conversations
- **Success Rate**: 98%

All metrics scored on a **1-5 scale** based on observable behaviors in conversation transcripts.
            """)

        with gr.Tab("üìà Metric Distributions"):
            gr.Markdown("### Distribution of All Learning Metrics")
            gr.Markdown("Explore the distribution of each metric across all 397 conversations. Red dashed lines indicate mean values.")
            gr.Plot(create_distribution_plot())

        with gr.Tab("üîó Correlations"):
            gr.Markdown("### Correlation Matrix")
            gr.Markdown("""
Strong positive correlations (r > 0.6) indicate metrics that tend to co-occur:
- **Cognitive Engagement ‚Üî Learning Quality**: r=0.72
- **Agency/Ownership ‚Üî Self-Directedness**: r=0.68
- **Query Sophistication ‚Üî Cognitive Engagement**: r=0.65
            """)
            gr.Plot(create_correlation_heatmap())

        with gr.Tab("üß† Cognitive Debt"):
            gr.Markdown("### Cognitive Debt Indicators (MIT 2025 Framework)")
            gr.Markdown("""
Based on the MIT Media Lab (2025) study on cognitive debt in AI-assisted tasks:

**Indicators**:
- **Iterative Refinement** (2.91/5): How deeply learners follow up on initial queries
- **Memory Retention** (3.68/5): Evidence of retaining and applying prior knowledge
- **Metacognitive Awareness** (3.22/5): Self-reflection and learning strategy awareness

**Finding**: Learners show **moderate engagement patterns** with **above-average memory retention** and metacognitive awareness, suggesting balanced AI utilization rather than over-dependency.
            """)
            gr.Plot(create_cognitive_debt_plot())

        with gr.Tab("üë• Learner Profiles"):
            gr.Markdown("### Learner Clustering (PCA + K-means)")
            gr.Markdown("""
Using Principal Component Analysis and K-means clustering, we identified **4 distinct learner profiles**:

1. **Low Engagers (25%)**: Minimal metrics across all dimensions - brief, surface-level interactions
2. **Moderate Balanced (38%)**: Average scores with balanced approach to different query types
3. **High Agency (20%)**: Strong self-direction and ownership - metacognitive and strategic
4. **Production Focused (17%)**: Higher Esperanto usage in messages, less reliance on translation

These profiles reveal heterogeneous learning strategies when using AI for language acquisition.
            """)
            gr.Plot(create_cluster_plot())

        with gr.Tab("üîç Relationships"):
            gr.Markdown("### Key Metric Relationships")
            gr.Markdown("""
Scatterplots showing the strongest relationships between metrics. Trend lines (red dashed) indicate direction of correlation.

**Key insights**:
- Higher cognitive engagement strongly predicts better overall learning quality
- Learners with strong agency show greater self-directedness
- Linguistic production correlates with more sophisticated queries
- Iterative refinement supports better memory retention
            """)
            gr.Plot(create_relationships_plot())

        with gr.Tab("üéØ Learning Patterns"):
            gr.Markdown("### Behavioral Patterns Analysis")
            gr.Markdown("""
**Declarative Bias**: 50% of learners favor fact/translation requests over procedural understanding (grammar, usage rules)

**AI Dependency Distribution**:
- 48% show balanced autonomy
- 32% show high reliance on AI
- 20% show high self-direction

**Query Type Distribution**:
- 40-45% translation queries
- 25-30% grammar queries
- <5% application/practice queries
- <2% meta-learning queries

**Esperanto Production**: Mean 38% usage in messages - most interaction remains English-based
            """)
            gr.Plot(create_learning_patterns_plot())

        with gr.Tab("üîé Conversation Explorer"):
            gr.Markdown("### Explore Individual Conversations")
            gr.Markdown(f"Enter a conversation index (0 to {len(df)-1}) to see detailed metrics and interpretation.")

            with gr.Row():
                conv_input = gr.Textbox(
                    label="Conversation Index",
                    placeholder="e.g., 0, 42, 150",
                    value="0"
                )
                explore_btn = gr.Button("Explore Conversation", variant="primary")

            conv_output = gr.Markdown()

            explore_btn.click(
                fn=explore_conversation,
                inputs=conv_input,
                outputs=conv_output
            )

        with gr.Tab("üìö About"):
            gr.Markdown("""
## About This Study

This dataset represents a comprehensive analysis of AI-assisted language learning, specifically focusing on Esperanto learners using ChatGPT. The study bridges behavioral economics, educational psychology, and natural language processing to understand how learners engage with Large Language Models for skill acquisition.

### Research Questions

1. How do learners interact with AI for language learning?
2. What patterns of cognitive engagement emerge in AI-assisted learning?
3. Do learners show signs of cognitive debt when using AI tutors?
4. What learner profiles can be identified from behavioral patterns?

### Dataset

- **Size**: 397 conversations from 375 unique participants
- **Timeframe**: December 2024
- **Collection**: ChatGPT export data (CSN format)
- **Processing**: Automated evaluation using GPT-3.5-turbo with validated frameworks
- **Variables**: 44 columns including metadata, behavioral metrics, and evaluation scores

### Validated Frameworks

All evaluation metrics are grounded in peer-reviewed assessment instruments:

- **Klepsch et al. (2017)**: Cognitive load measurement (Frontiers in Psychology, 8, 1997)
- **Schraw & Dennison (1994)**: Metacognitive Awareness Inventory (Contemporary Educational Psychology, 19(4), 460-475)
- **Pintrich et al. (1991)**: Motivated Strategies for Learning Questionnaire (MSLQ Manual, University of Michigan)
- **ACTFL (2024)**: ACTFL Proficiency Guidelines 2024
- **Kahu et al. (2018)**: Higher Education Student Engagement Scale (Research in Higher Education, 60(2), 134-159)
- **Liu et al. (2014)**: Critical thinking assessment (ETS Research Report Series)
- **MIT Media Lab (2025)**: Cognitive debt in AI-assisted tasks (arXiv:2506.08872)

See `analysis/LITERATURE_REVIEW_COMPREHENSIVE.md` for complete bibliography of 20+ papers.

### Citation

If you use this dataset in your research, please cite:

```bibtex
@dataset{esperanto_chatgpt_2024,
  title={Esperanto Learning with ChatGPT: A Behavioral Economics Study of AI-Assisted Language Learning},
  author={Your Name},
  year={2024},
  publisher={HuggingFace},
  url={https://github.com/lukketsvane/esperanto-db}
}
```

### Related Work

**MIT Cognitive Debt Study**:
MIT Media Lab (2025). Your Brain on ChatGPT: Accumulation of cognitive debt when using an AI assistant for essay writing task. *arXiv:2506.08872*

### Repository

Full source code, analysis scripts, and raw data available at:
**https://github.com/lukketsvane/esperanto-db**

### License

Dataset: MIT License
Code: MIT License

### Contact

For questions or collaborations, please open an issue on GitHub.
            """)

    gr.Markdown("""
---
<div style="text-align: center; color: #666; font-size: 0.9em;">
    <p>Built with Gradio and Plotly | Data Science meets Language Learning</p>
    <p>¬© 2024 | MIT License</p>
</div>
    """)

if __name__ == "__main__":
    demo.launch()
