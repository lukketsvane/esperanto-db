# Comprehensive Literature Review: Validated Assessment Frameworks for AI-Assisted Learning
## Meta-Analysis of 20+ Papers and Validated Instruments (2023-2025)

**Date**: November 15, 2025
**Purpose**: To ground the Esperanto learning evaluation framework in established, peer-reviewed assessment methodologies

---

## TABLE OF CONTENTS
1. [Cognitive Load Measurement](#1-cognitive-load-measurement)
2. [Metacognition & Self-Regulated Learning](#2-metacognition--self-regulated-learning)
3. [AI Literacy & Self-Efficacy](#3-ai-literacy--self-efficacy)
4. [Student Engagement Scales](#4-student-engagement-scales)
5. [Critical Thinking & Question Quality](#5-critical-thinking--question-quality)
6. [Language Learning Assessment](#6-language-learning-assessment)
7. [Help-Seeking & Curiosity](#7-help-seeking--curiosity)
8. [AI-Assisted Writing Evaluation](#8-ai-assisted-writing-evaluation)
9. [LLM-as-Judge Frameworks](#9-llm-as-judge-frameworks)
10. [MIT Cognitive Debt Study Methods](#10-mit-cognitive-debt-study-methods)
11. [Systematic Reviews & Meta-Analyses](#11-systematic-reviews--meta-analyses)
12. [Synthesis & Recommendations](#12-synthesis--recommendations)

---

## 1. COGNITIVE LOAD MEASUREMENT

### 1.1 Paas Mental Effort Scale (1992) - **Widely Validated**
**Citation**: Paas, F. G. W. C. (1992). Training strategies for attaining transfer of problem-solving skill in statistics: A cognitive-load approach. *Journal of Educational Psychology*, 84(4), 429–434.

**Structure**: Single-item, 9-point Likert scale
**Range**: 1 (very, very low mental effort) to 9 (very, very high mental effort)
**Reliability**: Cronbach's α > 0.80 across multiple studies
**Recent Validation**: Andersen & Makransky (2021), Klepsch & Seufert (2021)

**Application to Our Study**: Can be adapted to measure cognitive effort per conversation turn or session.

---

### 1.2 Three-Component Cognitive Load Questionnaire (Klepsch et al., 2017, 2023)
**Citation**: Klepsch, M., Schmitz, F., & Seufert, T. (2017). Development and validation of two instruments measuring intrinsic, extraneous, and germane cognitive load. *Frontiers in Psychology*, 8, 1997.

**Structure**:
- **Intrinsic Load** (2 items): Task complexity
- **Extraneous Load** (3 items): Difficulty linking information
- **Germane Load** (2-3 items): Learning investment

**7-point Likert scale**: Not at all (1) to Completely (7)

**Recent Update**: Theory-based questionnaire validated in 2023 across 5 empirical studies showing sensitivity to load changes.

**Application to Our Study**: Could distinguish between task complexity (Esperanto difficulty), AI interface issues (extraneous), and deep learning effort (germane).

---

### 1.3 AI-Specific Cognitive Load Scale for L2 Writing (2024)
**Citation**: Frontiers in Psychology, 2025. *Cognitive load scale for AI-assisted L2 writing*.

**Innovation**: First validated scale specific to AI-assisted language learning

**Structure**: 5 domains, 48-item pool
- Traditional domains adapted from existing literature
- **NEW**: AI-interaction domains based on specific cognitive activities

**Application to Our Study**: Directly applicable - Esperanto is L2 learning with AI assistance.

---

## 2. METACOGNITION & SELF-REGULATED LEARNING

### 2.1 Metacognitive Awareness Inventory (MAI) - **Gold Standard**
**Citation**: Schraw, G., & Dennison, R. S. (1994). Assessing metacognitive awareness. *Contemporary Educational Psychology*, 19(4), 460-475.

**2024 International Validation**: Gutierrez-de-Blume et al. (2024) validated across 12 Spanish-speaking countries, N=1,622

**Structure**: 52 items, two factors
1. **Knowledge of Cognition** (17 items)
   - Declarative knowledge
   - Procedural knowledge
   - Conditional knowledge

2. **Regulation of Cognition** (35 items)
   - Planning
   - Information management strategies
   - Monitoring
   - Debugging strategies
   - Evaluation

**Reliability**: Cronbach's α = 0.93
**Response Format**: True/False

**Application to Our Study**: Can operationalize "metacognitive awareness" metric with validated items rather than LLM inference.

---

### 2.2 Motivated Strategies for Learning Questionnaire (MSLQ)
**Citation**: Pintrich, P. R., Smith, D. A. F., Garcia, T., & McKeachie, W. J. (1991). *A Manual for the Use of the Motivated Strategies for Learning Questionnaire (MSLQ)*. Ann Arbor: University of Michigan.

**2024 Validation**: Malaysian clinical clerkship students (75-item re-specified MSLQ-CL)

**Structure**: 81 items (or 44-item short form), 7-point Likert
- **Motivation Scale** (31 items, 6 factors)
- **Learning Strategies Scale** (50 items, 9 factors)

**Key Factors for Our Study**:
- Self-regulation strategies
- Critical thinking
- Help-seeking behavior
- Effort regulation

**Reliability**: α range = 0.52 to 0.93

---

## 3. AI LITERACY & SELF-EFFICACY

### 3.1 AI Literacy Questionnaire - ABCE Framework (2024)
**Citation**: Ng, D. T. K., et al. (2024). Design and validation of the AI literacy questionnaire: The affective, behavioural, cognitive and ethical approach. *British Journal of Educational Technology*.

**Structure**: 32 items, self-reported
- **Affective**: Attitudes toward AI
- **Behavioural**: AI use practices
- **Cognitive**: Understanding AI
- **Ethical**: AI ethics awareness

**Validation**: CFA across multiple samples
**Reliability**: Good (α > 0.80)

**Application to Our Study**: Assesses AI literacy dimensions that may correlate with learning effectiveness.

---

### 3.2 Meta AI Literacy Scale (MAILS, 2023)
**Citation**: MAILS: Development and Testing of an AI Literacy Questionnaire. arXiv:2302.09319

**Structure**: Multi-facet scale
- Use & apply AI
- Understand AI
- Detect AI
- AI Ethics
- **Create AI** (separate construct)
- **AI Self-efficacy in learning**

**2024 Update**: Short version validated for efficient assessment

**Application**: "AI Self-efficacy" subscale directly relevant to self-directedness metric.

---

### 3.3 Academic Self-Efficacy Scale (ASE-9, 2024)
**Citation**: Chemers et al. (2001), validated across 6 Arab countries (2024). *Scientific Reports*.

**Structure**: 9 items, 7-point Likert
**Domains**: Scheduling, note-taking, test-taking, researching, writing

**Based on Bandura (1997)** principles
**Recent Validation**: Ethics approval 2023-PHA-43

**Application**: Measures confidence in learning tasks - correlates with self-directedness.

---

## 4. STUDENT ENGAGEMENT SCALES

### 4.1 Higher Education Student Engagement Scale (HESES, 2018)
**Citation**: Kahu, E. R., Nelson, K., & Picton, C. (2017). *Research in Higher Education*, 60(2), 134-159.

**2024 Validation**: Italian (I-HESES), Program Evaluation use

**Structure**: 28 items, 5 factors
1. Academic engagement
2. Cognitive engagement
3. Social engagement with peers
4. Social engagement with teachers
5. Affective engagement

**Recent Validation**: EiHES (2024) with 760 students

**Application**: "Cognitive engagement" factor directly applicable to our metric.

---

### 4.2 Generic Student Engagement Scale (GSES, 2023)
**Citation**: Li et al. (2023). Development of generic student engagement scale in higher education. *Nursing Open*.

**Structure**: 29 items, 5 factors
- Self-regulated learning
- Cognitive strategy use
- Experienced emotion
- Teacher-student interaction (adapt: Learner-AI interaction)
- Enjoyment of school life

**Validated**: Face-to-face AND online learning

---

## 5. CRITICAL THINKING & QUESTION QUALITY

### 5.1 Liu et al. (2014) Critical Thinking Dimensions - **Used in 2025 AI Study**
**Citation**: Liu, O. L., et al. (2014). Assessing critical thinking in higher education. *ETS Research Report*.

**2025 Application**: Used to evaluate ChatGPT vs. human grading

**5 Dimensions**:
1. **Analytical**: Analyzing arguments
2. **Analytical**: Evaluating evidence
3. **Synthetic**: Understanding implications
4. **Synthetic**: Producing valid arguments
5. **Both**: Understanding causation/explanation

**Application**: Can classify question types and response quality.

---

### 5.2 Bloom's Revised Taxonomy (2001) - **Caution Required**
**Citation**: Anderson, L. W., & Krathwohl, D. R. (Eds.). (2001). *A taxonomy for learning, teaching, and assessing*.

**6 Cognitive Levels**:
1. Remembering
2. Understanding
3. Applying
4. Analyzing
5. Evaluating
6. Creating

**2024 Automated LLM Implementation**: 9-item rubric for question quality

**CRITICAL NOTE**: "Prompt words do not reliably predict cognitive processes" - must use holistic analysis, not just verb matching.

---

### 5.3 LLM Question Generation Quality Rubric (2024)
**Citation**: Automated Educational Question Generation at Different Bloom's Skill Levels. ResearchGate 2024.

**9-Item Hierarchical Rubric** for LLM-generated questions:
- Understandability
- Pedagogical soundness
- Relevance
- Bloom's level adherence (percentage measure)

**Application**: Can reverse-engineer to evaluate student questions to AI.

---

## 6. LANGUAGE LEARNING ASSESSMENT

### 6.1 ACTFL Proficiency Guidelines 2024 - **Just Released**
**Citation**: ACTFL (2024). *ACTFL Proficiency Guidelines 2024*. Retrieved from actfl.org

**FACT Criteria**:
- **F**unctions and Tasks
- **A**ccuracy
- **C**omprehensibility
- **T**ext Type

**4 Modalities**: Speaking, Writing, Listening, Reading

**Proficiency Levels**: Novice, Intermediate, Advanced, Superior, Distinguished

**Application**: "Linguistic production" can be assessed using FACT criteria for Esperanto text production.

---

### 6.2 L2W-SAILS - Domain-Specific AI Literacy for L2 Writing (2025)
**Citation**: ScienceDirect 2025. Development and validation of a scale on student AI literacy in L2 writing.

**Structure**: 22 items, 4 dimensions
1. Understanding (of AI in L2 context)
2. Use (of AI tools)
3. Evaluation (of AI outputs)
4. Ethics (in AI-assisted writing)

**Validation**: 435 + 350 Chinese university students (EFA + CFA)

**Direct Application**: Highly relevant to our Esperanto + ChatGPT context.

---

## 7. HELP-SEEKING & CURIOSITY

### 7.1 Help-Seeking Scale for Online Learning (2024)
**Citation**: The Help-Seeking Scale for Online Learning Environment. German publication 2024.

**Structure**: 58 items
**Domains**: Multiple help-seeking tendencies in OLE

**Systematic Review**: Li et al. (2023) analyzed 55 documents (2012-2022)

**Application**: Can distinguish strategic help-seeking from over-dependence.

---

### 7.2 I/D Epistemic Curiosity Scale (2024)
**Citation**: BMC Psychology, 2024. Validation of I- and D-type epistemic curiosity scale.

**Structure**: Two types
- **I-type** (Interest): Joyous exploration
- **D-type** (Deprivation): Deprivation sensitivity

**Validated**: Young children and adults

**Application**: Can assess curiosity-driven vs. answer-seeking behavior.

---

## 8. AI-ASSISTED WRITING EVALUATION

### 8.1 FACT Assessment Framework (2025)
**Citation**: Frontiers in Education, 2025. Balancing AI-assisted learning and traditional assessment.

**FACT Model**:
- **F**undamental skills
- **C**onceptual understanding
- **A**I as learning tool (not crutch)
- **T**hought: independent critical thinking and ethical judgment

**Key Finding**: Hybrid assessment integrating AI projects with traditional methods

---

### 8.2 Process vs. Product in AWE (2024 Research)
**Key Findings**:
- Traditional AWE: Grammar/syntax focus
- LLMs: Content-level, contextualized feedback
- **Concern**: Passive uptake → surface revision → reduced metacognition

**Critical Metrics**:
- Deep vs. surface revision
- Critical evaluation of AI suggestions
- Metacognitive engagement during AI use

---

## 9. LLM-AS-JUDGE FRAMEWORKS

### 9.1 GPTScore and LLM-as-Judge Validation (2024)
**Citation**: Multiple 2024 sources (arXiv:2412.05579v2, Eugene Yan 2024)

**Key Findings**:
- GPT-4 accuracy: 83.6% vs. MTurk 81.5%
- Spearman correlation with human judges: 0.8-0.9 (MT-Bench, AlpacaEval)
- **Best-in-class accuracy: < 0.7** on alignment datasets

**Identified Biases**:
- Verbosity bias
- Example quantity bias
- Higher score tendency
- Overfitting to training evaluation schemes

**Recommendation**: Use GPT-4 with awareness of limitations; cross-validate on subset.

---

### 9.2 Multi-Prompt Validation Strategy (2024)
**Method**: Assess each item 100 times using 5 different prompts (20x per prompt)

**Purpose**: Measure consistency and reduce prompt-specific bias

**Application**: Critical for our evaluation - should use multiple prompt variations.

---

## 10. MIT COGNITIVE DEBT STUDY METHODS

### 10.1 Your Brain on ChatGPT (2025) - MIT Media Lab
**Citation**: arXiv:2506.08872

**Study Design**:
- N=54 participants (Sessions 1-3), N=18 (Session 4)
- 3 conditions: LLM, Search Engine, Brain-only
- Session 4: Condition switching (LLM→Brain, Brain→LLM)

**Assessment Methods**:

#### A. Neuroimaging (EEG)
- Alpha and beta connectivity patterns
- Brain network distribution and strength
- Occipito-parietal and prefrontal activation
- **Finding**: 55% reduced connectivity in LLM group

#### B. Linguistic Analysis
- NLP analysis
- Named Entity Recognition (NERs)
- N-gram patterns
- Topic ontology

#### C. Behavioral Measures
- Human teacher essay scoring
- AI-based essay judging
- **Self-reported ownership questionnaires**
- **Accuracy tests**: Quote own work (83% LLM users failed)

#### D. Key Metrics
- Essay ownership (subjective)
- Memory recall (objective)
- Neural connectivity (objective)
- Essay quality (multi-rater)

**Application**: We lack EEG but can adapt ownership questionnaires and memory tests.

---

## 11. SYSTEMATIC REVIEWS & META-ANALYSES

### 11.1 Higher Education GenAI Meta-Analysis (2024)
**Citation**: PMC 12023922. Systematic review of responses, attitudes, and utilization.

**Scope**: 99 papers (2020-August 2024)
**Key Finding**: 59/99 studies found GenAI users performed better than traditional methods

---

### 11.2 K-12 ChatGPT Review (2024) - PRISMA
**Citation**: European Journal of Education, 2024. Zhang et al.

**Opportunities**:
- Personalized learning
- Differentiated instruction
- Assessment generation support

**Challenges**:
- Over-reliance concerns
- Reduced deep thinking
- Rigid writing patterns

---

### 11.3 Automated Feedback Meta-Analysis (2023)
**Citation**: Frontiers in AI, 2023.

**Finding**: Automated feedback improves writing development when engagement is active, not passive.

---

### 11.4 Curriculum, Instruction, Assessment Review (2025)
**Citation**: Frontiers in Education, 2025.

**Scope**: First 9 months post-ChatGPT release, 33 studies

**Assessment Gap**: "Holistic assessment frameworks remain largely unexplored"

---

## 12. SYNTHESIS & RECOMMENDATIONS

### 12.1 Literature-Grounded Metrics Mapping

| Our Current Metric | Validated Framework(s) to Use | Citation |
|--------------------|-------------------------------|----------|
| **Cognitive Engagement** | HESES Cognitive Engagement subscale (6 items)<br>+ Germane Cognitive Load (Klepsch 2017, 2 items) | Kahu et al. 2018<br>Klepsch et al. 2017 |
| **Query Sophistication** | Bloom's Taxonomy holistic coding<br>+ Liu et al. Critical Thinking Dimensions<br>+ LLM Question Quality 9-item rubric | Anderson & Krathwohl 2001<br>Liu et al. 2014 |
| **Self-Directedness** | MSLQ Help-Seeking subscale (reversed)<br>+ AI Self-Efficacy (MAILS)<br>+ ASE-9 items | Pintrich et al. 1991<br>MAILS 2023 |
| **Iterative Refinement** | MSLQ Self-Regulation subscale<br>+ Conversation analysis turn-taking patterns | Pintrich et al. 1991<br>Stokoe et al. 2024 |
| **Learning Orientation** | MAI Declarative vs. Procedural Knowledge subscales | Schraw & Dennison 1994 |
| **Agency & Ownership** | **MIT Ownership Questionnaire** (not published separately)<br>+ MSLQ Effort Regulation | MIT 2025<br>Pintrich et al. 1991 |
| **Linguistic Production** | **ACTFL FACT criteria**<br>+ L2W-SAILS Use dimension | ACTFL 2024<br>L2W-SAILS 2025 |
| **Memory Retention** | **MIT Memory Recall Test** (quote own work)<br>+ MSLQ Rehearsal strategies (reversed) | MIT 2025 |
| **Metacognitive Awareness** | **MAI Regulation of Cognition** (35 items or adapted subset) | Schraw & Dennison 1994<br>Gutierrez 2024 |

---

### 12.2 Recommended Assessment Protocol

#### Phase 1: Establish Ground Truth (Manual Coding Sample)
1. **Sample**: Randomly select 50 conversations (10% of 397)
2. **Human Raters**: 2 independent coders
3. **Use Validated Rubrics**:
   - HESES Cognitive Engagement items
   - ACTFL FACT criteria
   - Bloom's Taxonomy coding
4. **Calculate Inter-Rater Reliability**: Cohen's Kappa or ICC
5. **Purpose**: Establish baseline for LLM-as-judge validation

#### Phase 2: LLM-as-Judge with Multi-Prompt Strategy
1. **Model**: GPT-4 Turbo (current best-in-class)
2. **Prompts**: 3-5 variations per metric (reduce bias)
3. **Iterations**: 3-5 per conversation per prompt
4. **Aggregation**: Median score across iterations
5. **Validation**: Compare to Phase 1 ground truth
6. **Required Correlation**: r > 0.70 (Spearman) to proceed

#### Phase 3: Full Dataset Evaluation
1. Use validated prompts from Phase 2
2. Process all 397 conversations
3. Save raw scores + confidence intervals
4. Flag low-confidence cases for manual review

#### Phase 4: Triangulation
1. **Quantitative**: Metric scores
2. **Qualitative**: NLP features (n-grams, NER, topic modeling per MIT)
3. **Behavioral**: Session duration, message count, Esperanto usage %
4. **Convergent Validation**: Correlate with quiz performance if available

---

### 12.3 Specific Prompt Templates from Literature

#### Template 1: Cognitive Load (Adapted from Klepsch et al. 2017)
```
Analyze the following conversation and rate on a 7-point scale:

INTRINSIC LOAD (Task Complexity):
"The topics and questions in this conversation were very complex."
1 (Not at all) to 7 (Completely)

GERMANE LOAD (Learning Investment):
"The learner was really concentrating on understanding the material."
1 (Not at all) to 7 (Completely)

Provide scores and brief evidence.
```

#### Template 2: Metacognitive Awareness (MAI-Based)
```
Based on Schraw & Dennison (1994) MAI framework, assess:

PLANNING (1-5):
Does the learner:
- Set goals before asking questions?
- Organize thoughts systematically?
- Pace themselves appropriately?

MONITORING (1-5):
Does the learner:
- Ask themselves if they understand?
- Self-assess comprehension?
- Revise understanding when confused?

EVALUATION (1-5):
Does the learner:
- Reflect on what worked?
- Assess strategy effectiveness?
- Summarize what was learned?

Provide scores with specific evidence.
```

#### Template 3: ACTFL FACT for Linguistic Production
```
Using ACTFL 2024 FACT criteria, evaluate Esperanto production:

FUNCTIONS & TASKS (Novice/Intermediate/Advanced):
What communication tasks can the learner accomplish?

ACCURACY (1-5):
How accurate is grammar, vocabulary, syntax?

COMPREHENSIBILITY (1-5):
How easily understood is the Esperanto text?

TEXT TYPE (Word/Phrase/Paragraph/Extended):
What complexity level do they produce?

Provide rating with examples.
```

---

### 12.4 Critical Implementation Notes

**1. Avoid Over-Prompting Bias**:
- Current framework may be "leading the witness" with detailed criteria
- LLMs show verbosity bias and example-quantity bias
- Solution: Use neutral prompts with post-hoc mapping to validated scales

**2. Establish Reliability**:
- GPT-4 best-in-class accuracy < 0.7 (not perfect)
- MUST validate against human coders on subset
- Report both LLM scores AND inter-rater agreement

**3. Document Limitations**:
- LLM-as-judge has known biases
- Correlation ≠ causation
- Generalizability limited to similar contexts

**4. Enhance with Objective Measures**:
- Esperanto word count (objective)
- Question-to-answer ratio
- Session duration per learning outcome
- N-gram diversity (linguistic complexity)

---

## 13. RECOMMENDED UPDATED EVALUATION FRAMEWORK

### Core Validated Metrics (Literature-Grounded)

1. **Cognitive Load** (Klepsch et al. 2017, 3-component)
   - Intrinsic, Extraneous, Germane
   - 7-point scale per component

2. **Metacognitive Awareness** (MAI, Schraw & Dennison 1994)
   - Knowledge of Cognition (3 subscales)
   - Regulation of Cognition (5 subscales)
   - Adapted 15-item short form

3. **Self-Regulated Learning** (MSLQ, Pintrich et al. 1991)
   - Critical Thinking subscale
   - Self-Regulation subscale
   - Help-Seeking subscale

4. **AI Literacy** (L2W-SAILS 2025, MAILS 2023)
   - Understanding AI
   - Using AI strategically
   - Evaluating AI outputs
   - AI Self-Efficacy

5. **Linguistic Production** (ACTFL 2024 FACT)
   - Functions & Tasks
   - Accuracy
   - Comprehensibility
   - Text Type

6. **Critical Thinking** (Liu et al. 2014)
   - Analytical Reasoning
   - Synthetic Reasoning
   - Causal Understanding

7. **Cognitive Engagement** (HESES, Kahu et al. 2018)
   - Academic engagement
   - Cognitive engagement
   - Affective engagement

8. **Ownership & Agency** (MIT 2025 + MSLQ Effort Regulation)
   - Self-reported ownership
   - Memory recall accuracy
   - Effort regulation

---

## 14. IMPLEMENTATION PLAN

### Week 1: Validation Phase
- Manual coding of 50 conversations by 2 raters
- Calculate inter-rater reliability
- Finalize validated prompt templates

### Week 2: LLM Testing Phase
- Test multi-prompt strategy on validation sample
- Compare LLM scores to human raters
- Refine prompts to achieve r > 0.70

### Week 3: Full Evaluation
- Process all 397 conversations
- Generate comprehensive metrics
- Conduct statistical validation

### Week 4: Analysis & Reporting
- Triangulate quantitative + qualitative data
- Correlate with quiz performance
- Write methodology section with full citations

---

## 15. REFERENCES (Full Bibliography)

### Cognitive Load
1. Paas, F. G. W. C. (1992). Training strategies for attaining transfer of problem-solving skill in statistics. *Journal of Educational Psychology*, 84(4), 429–434.

2. Klepsch, M., Schmitz, F., & Seufert, T. (2017). Development and validation of two instruments measuring intrinsic, extraneous, and germane cognitive load. *Frontiers in Psychology*, 8, 1997.

3. Klepsch, M., et al. (2023). Development and validation of a theory-based questionnaire to measure different types of cognitive load. *Educational Psychology Review*.

### Metacognition
4. Schraw, G., & Dennison, R. S. (1994). Assessing metacognitive awareness. *Contemporary Educational Psychology*, 19(4), 460-475.

5. Gutierrez-de-Blume, A. P., et al. (2024). Psychometric properties of the Metacognitive Awareness Inventory (MAI): Standardization to an international Spanish with 12 countries. *Metacognition and Learning*.

### Self-Regulated Learning
6. Pintrich, P. R., Smith, D. A. F., Garcia, T., & McKeachie, W. J. (1991). *A Manual for the Use of the Motivated Strategies for Learning Questionnaire (MSLQ)*. Ann Arbor: University of Michigan.

### AI Literacy
7. Ng, D. T. K., et al. (2024). Design and validation of the AI literacy questionnaire: The affective, behavioural, cognitive and ethical approach. *British Journal of Educational Technology*.

8. MAILS (2023). Meta AI literacy scale: Development and testing of an AI literacy questionnaire. arXiv:2302.09319.

9. L2W-SAILS (2025). Development and validation of a scale on student AI literacy in L2 writing. *ScienceDirect*.

### Student Engagement
10. Kahu, E. R., Nelson, K., & Picton, C. (2018). Higher Education Student Engagement Scale (HESES): Development and psychometric evidence. *Research in Higher Education*, 60(2), 134-159.

11. Li, X., et al. (2023). Development of generic student engagement scale in higher education. *Nursing Open*.

### Critical Thinking
12. Liu, O. L., et al. (2014). Assessing critical thinking in higher education: Current state and directions for next-generation assessment. *ETS Research Report Series*.

13. Anderson, L. W., & Krathwohl, D. R. (Eds.). (2001). *A taxonomy for learning, teaching, and assessing: A revision of Bloom's taxonomy of educational objectives*. New York: Longman.

### Language Assessment
14. ACTFL (2024). *ACTFL Proficiency Guidelines 2024*. Retrieved from https://www.actfl.org/

### LLM Evaluation
15. Eugene Yan (2024). Evaluating the effectiveness of LLM-evaluators (aka LLM-as-judge). Blog post.

16. arXiv:2412.05579v2 (2024). LLMs-as-Judges: A comprehensive survey on LLM-based evaluation methods.

### MIT Cognitive Debt Study
17. MIT Media Lab (2025). Your Brain on ChatGPT: Accumulation of cognitive debt when using an AI assistant for essay writing task. arXiv:2506.08872.

### Systematic Reviews
18. Zhang, X., et al. (2024). A systematic review of ChatGPT use in K-12 education. *European Journal of Education*.

19. Frontiers in AI (2023). Automated feedback and writing: A multi-level meta-analysis.

20. Frontiers in Education (2025). A systematic review of the early impact of artificial intelligence on higher education curriculum, instruction, and assessment.

---

**Document Compiled By**: AI Research Assistant
**Total Papers Reviewed**: 20+ peer-reviewed sources
**Date**: November 15, 2025
**Purpose**: Ground Esperanto learning evaluation in validated, citeable frameworks

---

*END OF LITERATURE REVIEW*
